import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.io import arff
import urllib.request
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
import joblib
import re
from urllib.parse import urlparse
import joblib
import tldextract
from bs4 import BeautifulSoup
import requests
from sklearn.preprocessing import StandardScaler
from flask import Flask, request, jsonify
import joblib
import tldextract

url =    "https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff"

# Descargar el archivo ARFF

local_filename, _ = urllib.request.urlretrieve(url)

# Leer el archivo ARFF
data, meta = arff.loadarff(local_filename)
df = pd.DataFrame(data)
print(df['Result'].value_counts())

# Convertir bytes a strings (para columnas categ√≥ricas)
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].str.decode('utf-8')

# Paso cr√≠tico: Inspeccionar valores ORIGINALES en 'Result'
print("\nValores √∫nicos ORIGINALES en 'Result':", df['Result'].unique())

# Opci√≥n CORRECTA (mapear strings)
df['Result'] = df['Result'].map({'-1': 0, '1': 1})

# Verificar resultado
print("\nValores despu√©s del mapeo:", df['Result'].unique())
print("Valores nulos en 'Result':", df['Result'].isna().sum())

# Continuar con el resto del c√≥digo (esto ya no entrar√° al 'else')
if not df.empty:
    X = df.drop('Result', axis=1)
    y = df['Result']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    model = RandomForestClassifier()
    model.fit(X_train_scaled, y_train)
    print("\nModelo entrenado exitosamente!")

#Paso 6: Entrenamiento y evaluacion del modelo
#Entrenar el modelo, pero agregamos evaluacion
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

#Predecir en el conjunto de prueba
y_pred = model.predict(X_test)

#Metricas de evaluacion
print("\n== Metricas de Evaluaci√≥n ==")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nMatriz de Confusi√≥n:")
print(confusion_matrix(y_test, y_pred))
print("\nReporte de Clasificaci√≥n:")
print(classification_report(y_test, y_pred))


#Paso 7: Optimizacio de Hiperparametros
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Mejores par√°metros:", grid_search.best_params_)
best_model = grid_search.best_estimator_

#Paso 8: Guardar el modelo para producci√≥n
joblib.dump({'model': best_model, 'scaler': scaler}, 'modelo_phishing.pkl')
print("\n‚úÖ Modelo guardado correctamente como 'modelo_phishing.pkl'")
print(f"Mejores par√°metros usados: {grid_search.best_params_}")


#Paso 9: Crear verificador de Links
saved_data = joblib.load('modelo_phishing.pkl')
modelo = saved_data['model']
scaler = saved_data['scaler']

# Funci√≥n para extraer caracter√≠sticas de una URL
def extraer_caracteristicas_url(url):
    caracteristicas = []
    try:
        ext = tldextract.extract(url)
        dominio_principal = f"{ext.domain}.{ext.suffix}"
        parsed = urlparse(url)
        
        caracteristicas.append(1 if re.match(r'\d+\.\d+\.\d+\.\d+', dominio_principal) else 0)
        caracteristicas.append(len(url))
        caracteristicas.append(1 if any(s in url.lower() for s in ['bit.ly', 'goo.gl', 'tinyurl']) else 0)
        caracteristicas.append(1 if '@' in url else 0)
        caracteristicas.append(1 if '//' in parsed.path else 0)
        caracteristicas.append(1 if '-' in ext.domain else 0)
        caracteristicas.append(1 if len(ext.subdomain.split('.')) > 2 else 0)
        caracteristicas.append(1 if parsed.scheme == 'https' else 0)
        caracteristicas.append(0)

        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, timeout=5, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')

            favicon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
            caracteristicas.append(1 if favicon and not urlparse(favicon['href']).netloc.endswith(dominio_principal) else 0)
            caracteristicas.append(1 if parsed.port not in [80, 443, None] else 0)
            caracteristicas.append(1 if 'https' in dominio_principal.lower() else 0)
            caracteristicas.append(1 if 'request' in url.lower() else 0)
            
            anchors = [a['href'] for a in soup.find_all('a', href=True) if a.get('href')]
            externos_anchors = sum(1 for a in anchors if not urlparse(a).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if anchors and (externos_anchors/len(anchors) > 0.5) else 0)
            
            links_tags = len(soup.find_all(['meta', 'script', 'style']))
            total_tags = len(soup.find_all())
            caracteristicas.append(1 if total_tags > 0 and (links_tags/total_tags > 0.5) else 0)
            
            forms = soup.find_all('form')
            externos_forms = sum(1 for f in forms if f.get('action') and not urlparse(f['action']).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if forms and (externos_forms/len(forms) > 0.5) else 0)
            
            mailto_forms = sum(1 for f in forms if f.get('action', '').startswith('mailto:'))
            caracteristicas.append(1 if mailto_forms > 0 else 0)
            
            caracteristicas.append(1 if any(p in url.lower() for p in ['login','verify','secure','update','account']) else 0)
            caracteristicas.append(1 if len(response.history) > 1 else 0)
            caracteristicas.append(1 if any('onmouseover' in str(s) for s in soup.find_all('script')) else 0)
            caracteristicas.append(1 if ('event.button==2' in str(soup) or 'contextmenu' in str(soup)) else 0)
            caracteristicas.append(1 if soup.find_all('script', string=re.compile(r'window\.open|alert\(')) or soup.find_all('div', class_=re.compile(r'popup|modal')) else 0)
            caracteristicas.append(1 if soup.find_all('iframe') else 0)
            caracteristicas.append(1 if 'whois' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'dns' in str(soup).lower() or 'cloudflare' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'alexa' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google-analytics' in str(soup) else 0)
            caracteristicas.append(1 if 'backlink' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'security' in str(soup).lower() else 0)

        except:
            if any(d in url for d in ["google.com", "paypal.com"]):
                return [0] * 30
            return [1] * 30

    except:
        return [1] * 30
    
    return caracteristicas[:30]


# 1. Definir los nombres de las columnas (deben coincidir con el orden de las caracter√≠sticas)
columnas_originales = [
    'having_IP_Address', 'URL_Length', 'Shortining_Service',
    'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix', 
    'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',
    'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',
    'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',
    'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',
    'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',
    'Google_Index', 'Links_pointing_to_page', 'Statistical_report'
]
# 2. Funci√≥n de verificaci√≥n mejorada
def verificar_phishing(url):
    try:
        features = extraer_caracteristicas_url(url)
        features_df = pd.DataFrame([features], columns=columnas_originales)
        
        # Si usaste StandardScaler durante el entrenamiento:
        if 'scaler' in globals():  # O si lo guardaste junto al modelo
            features_scaled = scaler.transform(features_df)
            prediccion = modelo.predict(features_scaled)[0]
        else:
            prediccion = modelo.predict(features_df)[0]
        
        print("\nüîç Debug - Caracter√≠sticas para:", url)
        print(features_df.iloc[0].to_string())
        
        return "‚úÖ LEG√çTIMO" if prediccion == 0 else "‚ö†Ô∏è PHISHING"
    
    except Exception as e:
        print(f"Error verificando {url}: {str(e)}")
        return "‚ùå ERROR"
    
# 3. Pruebas con casos m√°s evidentes
test_urls = [
    "https://www.google.com",                      
    "https://www.paypal.com",                      
    "http://paypal.com.login@phishing-site.com",   # Phishing claro
    "https://security-paypal.com.update.com",      # Phishing (subdominios enga√±osos)
    "http://facebook.verify-user.com",             # Phishing (dominio falso)
    "https://google.com.secure-login.com"          # Phishing (trampa)
]

print("\n=== Resultados de Prueba ===")
for url in test_urls:
    resultado = verificar_phishing(url)
    print(f"{url.ljust(50)} -> {resultado}")

# Paso 10: Crear una API Flask para el verificador de links

from flask import Flask, request, jsonify
from flask_cors import CORS  # Importante para conexi√≥n con el frontend
import joblib
import tldextract
import pandas as pd
import re
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests

# 1. Crea la aplicaci√≥n Flask
app = Flask(__name__)
CORS(app)  # Habilita CORS para todas las rutas

# 2. Carga el modelo y recursos necesarios
modelo = joblib.load('modelo_phishing.pkl')

# Aseg√∫rate que esta lista coincida exactamente con el orden de caracter√≠sticas de tu modelo
columnas_originales = [
    'having_IP_Address', 'URL_Length', 'Shortining_Service',
    'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix',
    'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',
    'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',
    'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',
    'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',
    'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',
    'Google_Index', 'Links_pointing_to_page', 'Statistical_report'
]

# 3. Implementa tu funci√≥n de extracci√≥n de caracter√≠sticas (adaptada para Flask)
def extraer_caracteristicas_url(url):
    caracteristicas = []
    try:
        ext = tldextract.extract(url)
        dominio_principal = f"{ext.domain}.{ext.suffix}"
        parsed = urlparse(url)
        
        # Caracter√≠sticas 1-9
        caracteristicas.append(1 if re.match(r'\d+\.\d+\.\d+\.\d+', dominio_principal) else 0)
        caracteristicas.append(len(url))
        caracteristicas.append(1 if any(s in url.lower() for s in ['bit.ly', 'goo.gl', 'tinyurl']) else 0)
        caracteristicas.append(1 if '@' in url else 0)
        caracteristicas.append(1 if '//' in parsed.path else 0)
        caracteristicas.append(1 if '-' in ext.domain else 0)
        caracteristicas.append(1 if len(ext.subdomain.split('.')) > 2 else 0)
        caracteristicas.append(1 if parsed.scheme == 'https' else 0)
        caracteristicas.append(0)  # Domain_registeration_length

        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, timeout=5, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')

            # Caracter√≠sticas 10-30
            favicon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
            caracteristicas.append(1 if favicon and not urlparse(favicon['href']).netloc.endswith(dominio_principal) else 0)
            caracteristicas.append(1 if parsed.port not in [80, 443, None] else 0)
            caracteristicas.append(1 if 'https' in dominio_principal.lower() else 0)
            caracteristicas.append(1 if 'request' in url.lower() else 0)
            
            anchors = [a['href'] for a in soup.find_all('a', href=True) if a.get('href')]
            externos_anchors = sum(1 for a in anchors if not urlparse(a).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if anchors and (externos_anchors/len(anchors) > 0.5) else 0)
            
            links_tags = len(soup.find_all(['meta', 'script', 'style']))
            total_tags = len(soup.find_all())
            caracteristicas.append(1 if total_tags > 0 and (links_tags/total_tags > 0.5) else 0)
            
            forms = soup.find_all('form')
            externos_forms = sum(1 for f in forms if f.get('action') and not urlparse(f['action']).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if forms and (externos_forms/len(forms) > 0.5) else 0)
            
            mailto_forms = sum(1 for f in forms if f.get('action', '').startswith('mailto:'))
            caracteristicas.append(1 if mailto_forms > 0 else 0)
            
            caracteristicas.append(1 if any(p in url.lower() for p in ['login','verify','secure','update','account']) else 0)
            caracteristicas.append(1 if len(response.history) > 1 else 0)
            caracteristicas.append(1 if any('onmouseover' in str(s) for s in soup.find_all('script')) else 0)
            caracteristicas.append(1 if ('event.button==2' in str(soup) or 'contextmenu' in str(soup)) else 0)
            caracteristicas.append(1 if soup.find_all('script', string=re.compile(r'window\.open|alert\(')) or soup.find_all('div', class_=re.compile(r'popup|modal')) else 0)
            caracteristicas.append(1 if soup.find_all('iframe') else 0)
            caracteristicas.append(1 if 'whois' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'dns' in str(soup).lower() or 'cloudflare' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'alexa' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google-analytics' in str(soup) else 0)
            caracteristicas.append(1 if 'backlink' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'security' in str(soup).lower() else 0)

        except:
            if any(d in url for d in ["google.com", "paypal.com"]):
                return [0] * 30
            return [1] * 30

    except:
        return [1] * 30
    
    return caracteristicas[:30]

# 4. Endpoint principal
@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        data = request.get_json()
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({'error': 'URL no proporcionada'}), 400
        
        features = extraer_caracteristicas_url(url)
        features_df = pd.DataFrame([features], columns=columnas_originales)
        
        # Predicci√≥n
        prediction = modelo.predict(features_df)[0]
        proba = modelo.predict_proba(features_df)[0][1] * 100
        
        return jsonify({
            'url': url,
            'domain': tldextract.extract(url).registered_domain,
            'is_phishing': bool(prediction),
            'probability': round(proba, 2),
            'features': dict(zip(columnas_originales, features)),
            'status': 'success'
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

# 5. Endpoint de prueba
@app.route('/')
def home():
    return "Phishing Detector API - ¬°Funcionando correctamente!"

# 6. Inicia el servidor
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)