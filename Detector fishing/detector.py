import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.io import arff
import urllib.request
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
import joblib
import re
from urllib.parse import urlparse
import joblib
import tldextract
from bs4 import BeautifulSoup
import requests
from sklearn.preprocessing import StandardScaler
from flask import Flask, request, jsonify
import joblib
import tldextract

url =    "https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff"

# Descargar el archivo ARFF

local_filename, _ = urllib.request.urlretrieve(url)

# Leer el archivo ARFF
data, meta = arff.loadarff(local_filename)
df = pd.DataFrame(data)
print(df['Result'].value_counts())

# Convertir bytes a strings (para columnas categÃ³ricas)
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].str.decode('utf-8')

# Paso crÃ­tico: Inspeccionar valores ORIGINALES en 'Result'
print("\nValores Ãºnicos ORIGINALES en 'Result':", df['Result'].unique())

# OpciÃ³n CORRECTA (mapear strings)
df['Result'] = df['Result'].map({'-1': 0, '1': 1})

# Verificar resultado
print("\nValores despuÃ©s del mapeo:", df['Result'].unique())
print("Valores nulos en 'Result':", df['Result'].isna().sum())

# Continuar con el resto del cÃ³digo (esto ya no entrarÃ¡ al 'else')
if not df.empty:
    X = df.drop('Result', axis=1)
    y = df['Result']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    model = RandomForestClassifier()
    model.fit(X_train_scaled, y_train)
    print("\nModelo entrenado exitosamente!")

#Paso 6: Entrenamiento y evaluacion del modelo
#Entrenar el modelo, pero agregamos evaluacion
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

#Predecir en el conjunto de prueba
y_pred = model.predict(X_test)

#Metricas de evaluacion
print("\n== Metricas de EvaluaciÃ³n ==")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nMatriz de ConfusiÃ³n:")
print(confusion_matrix(y_test, y_pred))
print("\nReporte de ClasificaciÃ³n:")
print(classification_report(y_test, y_pred))


#Paso 7: Optimizacio de Hiperparametros
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Mejores parÃ¡metros:", grid_search.best_params_)
best_model = grid_search.best_estimator_

#Paso 8: Guardar el modelo para producciÃ³n
joblib.dump({'model': best_model, 'scaler': scaler}, 'modelo_phishing.pkl')
print("\nâœ… Modelo guardado correctamente como 'modelo_phishing.pkl'")
print(f"Mejores parÃ¡metros usados: {grid_search.best_params_}")


#Paso 9: Crear verificador de Links
saved_data = joblib.load('modelo_phishing.pkl')
modelo = saved_data['model']
scaler = saved_data['scaler']

# FunciÃ³n para extraer caracterÃ­sticas de una URL
def extraer_caracteristicas_url(url):
    caracteristicas = []
    try:
        ext = tldextract.extract(url)
        dominio_principal = f"{ext.domain}.{ext.suffix}"
        parsed = urlparse(url)
        
        caracteristicas.append(1 if re.match(r'\d+\.\d+\.\d+\.\d+', dominio_principal) else 0)
        caracteristicas.append(len(url))
        caracteristicas.append(1 if any(s in url.lower() for s in ['bit.ly', 'goo.gl', 'tinyurl']) else 0)
        caracteristicas.append(1 if '@' in url else 0)
        caracteristicas.append(1 if '//' in parsed.path else 0)
        caracteristicas.append(1 if '-' in ext.domain else 0)
        caracteristicas.append(1 if len(ext.subdomain.split('.')) > 2 else 0)
        caracteristicas.append(1 if parsed.scheme == 'https' else 0)
        caracteristicas.append(0)

        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, timeout=5, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')

            favicon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
            caracteristicas.append(1 if favicon and not urlparse(favicon['href']).netloc.endswith(dominio_principal) else 0)
            caracteristicas.append(1 if parsed.port not in [80, 443, None] else 0)
            caracteristicas.append(1 if 'https' in dominio_principal.lower() else 0)
            caracteristicas.append(1 if 'request' in url.lower() else 0)
            
            anchors = [a['href'] for a in soup.find_all('a', href=True) if a.get('href')]
            externos_anchors = sum(1 for a in anchors if not urlparse(a).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if anchors and (externos_anchors/len(anchors) > 0.5) else 0)
            
            links_tags = len(soup.find_all(['meta', 'script', 'style']))
            total_tags = len(soup.find_all())
            caracteristicas.append(1 if total_tags > 0 and (links_tags/total_tags > 0.5) else 0)
            
            forms = soup.find_all('form')
            externos_forms = sum(1 for f in forms if f.get('action') and not urlparse(f['action']).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if forms and (externos_forms/len(forms) > 0.5) else 0)
            
            mailto_forms = sum(1 for f in forms if f.get('action', '').startswith('mailto:'))
            caracteristicas.append(1 if mailto_forms > 0 else 0)
            
            caracteristicas.append(1 if any(p in url.lower() for p in ['login','verify','secure','update','account']) else 0)
            caracteristicas.append(1 if len(response.history) > 1 else 0)
            caracteristicas.append(1 if any('onmouseover' in str(s) for s in soup.find_all('script')) else 0)
            caracteristicas.append(1 if ('event.button==2' in str(soup) or 'contextmenu' in str(soup)) else 0)
            caracteristicas.append(1 if soup.find_all('script', string=re.compile(r'window\.open|alert\(')) or soup.find_all('div', class_=re.compile(r'popup|modal')) else 0)
            caracteristicas.append(1 if soup.find_all('iframe') else 0)
            caracteristicas.append(1 if 'whois' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'dns' in str(soup).lower() or 'cloudflare' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'alexa' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google-analytics' in str(soup) else 0)
            caracteristicas.append(1 if 'backlink' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'security' in str(soup).lower() else 0)

        except:
            if any(d in url for d in ["google.com", "paypal.com"]):
                return [0] * 30
            return [1] * 30

    except:
        return [1] * 30
    
    return caracteristicas[:30]


# 1. Definir los nombres de las columnas (deben coincidir con el orden de las caracterÃ­sticas)
columnas_originales = [
    'having_IP_Address', 'URL_Length', 'Shortining_Service',
    'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix', 
    'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',
    'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',
    'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',
    'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',
    'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',
    'Google_Index', 'Links_pointing_to_page', 'Statistical_report'
]
# 2. FunciÃ³n de verificaciÃ³n mejorada
def verificar_phishing(url):
    try:
        features = extraer_caracteristicas_url(url)
        features_df = pd.DataFrame([features], columns=columnas_originales)
        
        # Si usaste StandardScaler durante el entrenamiento:
        if 'scaler' in globals():  # O si lo guardaste junto al modelo
            features_scaled = scaler.transform(features_df)
            prediccion = modelo.predict(features_scaled)[0]
        else:
            prediccion = modelo.predict(features_df)[0]
        
        print("\nðŸ” Debug - CaracterÃ­sticas para:", url)
        print(features_df.iloc[0].to_string())
        
        return "âœ… LEGÃTIMO" if prediccion == 0 else "âš ï¸ PHISHING"
    
    except Exception as e:
        print(f"Error verificando {url}: {str(e)}")
        return "âŒ ERROR"
    
# 3. Pruebas con casos mÃ¡s evidentes
test_urls = [
    "https://www.google.com",                      
    "https://www.paypal.com",                      
    "http://paypal.com.login@phishing-site.com",   # Phishing claro
    "https://security-paypal.com.update.com",      # Phishing (subdominios engaÃ±osos)
    "http://facebook.verify-user.com",             # Phishing (dominio falso)
    "https://google.com.secure-login.com"          # Phishing (trampa)
]

print("\n=== Resultados de Prueba ===")
for url in test_urls:
    resultado = verificar_phishing(url)
    print(f"{url.ljust(50)} -> {resultado}")

# Paso 10: Crear una API Flask para el verificador de links

from flask import Flask, request, jsonify
from flask_cors import CORS  # Importante para conexiÃ³n con el frontend
import joblib
import tldextract
import pandas as pd
import re
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests

# 1. Crea la aplicaciÃ³n Flask
app = Flask(__name__)
CORS(app)  # Habilita CORS para todas las rutas

# 2. Carga el modelo y recursos necesarios
modelo = joblib.load('modelo_phishing.pkl')

# AsegÃºrate que esta lista coincida exactamente con el orden de caracterÃ­sticas de tu modelo
columnas_originales = [
    'having_IP_Address', 'URL_Length', 'Shortining_Service',
    'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix',
    'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',
    'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',
    'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',
    'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',
    'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',
    'Google_Index', 'Links_pointing_to_page', 'Statistical_report'
]

# 3. Implementa tu funciÃ³n de extracciÃ³n de caracterÃ­sticas (adaptada para Flask)
def extraer_caracteristicas_url(url):
    caracteristicas = []
    try:
        ext = tldextract.extract(url)
        dominio_principal = f"{ext.domain}.{ext.suffix}"
        parsed = urlparse(url)
        
        # CaracterÃ­sticas 1-9
        caracteristicas.append(1 if re.match(r'\d+\.\d+\.\d+\.\d+', dominio_principal) else 0)
        caracteristicas.append(len(url))
        caracteristicas.append(1 if any(s in url.lower() for s in ['bit.ly', 'goo.gl', 'tinyurl']) else 0)
        caracteristicas.append(1 if '@' in url else 0)
        caracteristicas.append(1 if '//' in parsed.path else 0)
        caracteristicas.append(1 if '-' in ext.domain else 0)
        caracteristicas.append(1 if len(ext.subdomain.split('.')) > 2 else 0)
        caracteristicas.append(1 if parsed.scheme == 'https' else 0)
        caracteristicas.append(0)  # Domain_registeration_length

        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, timeout=5, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')

            # CaracterÃ­sticas 10-30
            favicon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
            caracteristicas.append(1 if favicon and not urlparse(favicon['href']).netloc.endswith(dominio_principal) else 0)
            caracteristicas.append(1 if parsed.port not in [80, 443, None] else 0)
            caracteristicas.append(1 if 'https' in dominio_principal.lower() else 0)
            caracteristicas.append(1 if 'request' in url.lower() else 0)
            
            anchors = [a['href'] for a in soup.find_all('a', href=True) if a.get('href')]
            externos_anchors = sum(1 for a in anchors if not urlparse(a).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if anchors and (externos_anchors/len(anchors) > 0.5) else 0)
            
            links_tags = len(soup.find_all(['meta', 'script', 'style']))
            total_tags = len(soup.find_all())
            caracteristicas.append(1 if total_tags > 0 and (links_tags/total_tags > 0.5) else 0)
            
            forms = soup.find_all('form')
            externos_forms = sum(1 for f in forms if f.get('action') and not urlparse(f['action']).netloc.endswith(dominio_principal))
            caracteristicas.append(1 if forms and (externos_forms/len(forms) > 0.5) else 0)
            
            mailto_forms = sum(1 for f in forms if f.get('action', '').startswith('mailto:'))
            caracteristicas.append(1 if mailto_forms > 0 else 0)
            
            caracteristicas.append(1 if any(p in url.lower() for p in ['login','verify','secure','update','account']) else 0)
            caracteristicas.append(1 if len(response.history) > 1 else 0)
            caracteristicas.append(1 if any('onmouseover' in str(s) for s in soup.find_all('script')) else 0)
            caracteristicas.append(1 if ('event.button==2' in str(soup) or 'contextmenu' in str(soup)) else 0)
            caracteristicas.append(1 if soup.find_all('script', string=re.compile(r'window\.open|alert\(')) or soup.find_all('div', class_=re.compile(r'popup|modal')) else 0)
            caracteristicas.append(1 if soup.find_all('iframe') else 0)
            caracteristicas.append(1 if 'whois' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'dns' in str(soup).lower() or 'cloudflare' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'alexa' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'google-analytics' in str(soup) else 0)
            caracteristicas.append(1 if 'backlink' in str(soup).lower() else 0)
            caracteristicas.append(1 if 'security' in str(soup).lower() else 0)

        except:
            if any(d in url for d in ["google.com", "paypal.com"]):
                return [0] * 30
            return [1] * 30

    except:
        return [1] * 30
    
    return caracteristicas[:30]

# 4. Endpoint principal
@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        data = request.get_json()
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({'error': 'URL no proporcionada'}), 400
        
        features = extraer_caracteristicas_url(url)
        features_df = pd.DataFrame([features], columns=columnas_originales)
        
        # PredicciÃ³n
        prediction = modelo.predict(features_df)[0]
        proba = modelo.predict_proba(features_df)[0][1] * 100
        
        return jsonify({
            'url': url,
            'domain': tldextract.extract(url).registered_domain,
            'is_phishing': bool(prediction),
            'probability': round(proba, 2),
            'features': dict(zip(columnas_originales, features)),
            'status': 'success'
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

# 5. Endpoint de prueba
@app.route('/')
def home():
    return "Phishing Detector API - Â¡Funcionando correctamente!"

# 6. Inicia el servidor
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)